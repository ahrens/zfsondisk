<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Matthew Ahrens &lt;mahrens@delphix.com&gt;">
  <title>ZFS On-disk Format</title>
  <style type="text/css">code{white-space: pre;}</style>
  <link rel="stylesheet" href="styles/pandoc.css">
  <link rel="stylesheet" href="styles/tufte-extra.css">
  <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
   var mathElements = document.getElementsByClassName("math");
   var macros = [];
   for (var i = 0; i < mathElements.length; i++) {
    var texText = mathElements[i].firstChild;
    if (mathElements[i].tagName == "SPAN") {
     katex.render(texText.data, mathElements[i], {
      displayMode: mathElements[i].classList.contains('display'),
      throwOnError: false,
      macros: macros,
      fleqn: false
     });
  }}});
  </script>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" />
<style type="text/css">
  /* CSS counter */
  html {
      counter-reset: ;
  }
  /* style of "before" */
  /** before plain or definition **/
  .proof:before
  {
      font-weight:700;
      font-style:normal
  }
  /** before remark or proof **/
  .proof:before
  {
      font-style:italic
  }
  /* content of "before" (everything goes here) */
              .proof:before
  {
      content:"证明：";
      font-family: "Kaiti SC";
      font-weight: bold;
      font-style: normal;
      text-decoration: underline;
  }
  /* QED */
  .proof:after
  {
      /* content: "\25FB"; */
      content: "【证毕】";
      float:right;
      font-family: "Kaiti SC";
      font-weight: bold;
      font-style: normal;
  }
  /* main style */
  /** plain **/
  ,.proof
  {
      display: block;
      margin:  12px 0;
      font-style:italic
  }
  /** definition, remark or proof **/
  ,.proof
  {
      display: block;
      margin:  12px 0;
      font-style:normal;
  }
</style>

  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->

</head>
<body>
<article>
<header>
<h1 class="title">ZFS On-disk Format</h1>
<!--
<p class="byline">Wed 02 Jun 2021 04:28:43 PM EDT &ndash; Matthew Ahrens &lt;mahrens@delphix.com&gt;</p>
-->
<p class="byline">Matthew Ahrens &lt;mahrens@delphix.com&gt;</p>
<p class="byline">Wed 02 Jun 2021 04:28:43 PM EDT</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#vdevs-labels-and-boot-block">Vdevs, Labels, and Boot Block</a>
<ul>
<li><a href="#virtual-devices">Virtual Devices</a></li>
<li><a href="#vdev-labels">Vdev Labels</a></li>
<li><a href="#sec:vdev_detail">Vdev Technical Details</a></li>
<li><a href="#boot-block">Boot Block</a></li>
</ul></li>
<li><a href="#block-pointers-and-indirect-blocks">Block Pointers and Indirect Blocks</a>
<ul>
<li><a href="#data-virtual-address">Data Virtual Address</a></li>
<li><a href="#grid">GRID</a></li>
<li><a href="#gang">GANG</a></li>
</ul></li>
</ul>
</nav>
<p><p><a href="zfs_internals.md.pdf">Click here</a> for a printer friendly PDF version.</p></p>
<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>ZFS is a new filesystem technology
that provides immense capacity (128-bit),
provable data integrity,
always-consistent on-disk format,
self-optimizing performance,
and real-time remote replication.</p>
<p>ZFS departs from traditional filesystems by eliminating the concept of volumes.
Instead,
ZFS filesystems share a common storage pool
consisting of writeable storage media.
Media can be added or removed from the pool
as filesystem capacity requirements change.
Filesystems dynamically grow and shrink as needed
without the need to re-partition underlying storage.</p>
<p>ZFS provides a truly consistent on-disk format,
but using a <em>copy on write</em> (<em>COW</em>) transaction model.
This model ensures that on disk data is never overwritten
and all on disk updates are done atomically.</p>
<p>The ZFS software is comprised of seven distinct pieces:
the <em>SPA</em> (<em>Storage Pool Allocator</em>),
the <em>DSL</em> (<em>Dataset and Snapshot Layer</em>),
the <em>DMU</em> (<em>Data Management Layer</em>),
the <em>ZAP</em> (<em>ZFS Attribute Processor</em>),
the _ ZPL_ (<em>ZFS Posix layer</em>),
the <em>ZIL</em> (<em>ZFS Intent Log</em>),
and <em>ZVOL</em> (<em>ZFS Volume</em>).
The on-disk structures associated with each of these pieces are explained in the following chapters:
SPA (Chapters 1 and 2),
DSL (Chapter 5),
DMU (Chapter 3),
ZAP (Chapter 4),
ZPL (Chapter 6),
ZIL (Chapter 7),
ZVOL (Chapter 8).</p>
</section>
<section id="vdevs-labels-and-boot-block" class="level1">
<h1>Vdevs, Labels, and Boot Block</h1>
<section id="virtual-devices" class="level2">
<h2>Virtual Devices</h2>
<p>ZFS storage pools are made up of a collection of virtual devices.
There are two types of virtual devices: physical virtual devices
(sometimes called leaf vdevs)
and logical virtual devices
(sometimes called interior vdevs).
A physical vdev,
is a writeable media block device (a disk, for example).
A logical vdev is a conceptual grouping of physical vdevs.</p>
<p>Vdevs are arranged in a tree with physical vdev existing as leaves of the tree.
All pools have a special logical vdev called the “root” vdev
which roots the tree.
All direct children of the “root” vdev (physical or logical) are called top-level vdevs.
Illustration. 1 shows a tree of vdevs
representing a sample pool configuration containing two mirrors.
The first mirror (labeled “M1”) contains two disk,
represented by “vdev A” and “vdev B”.
Likewise, the second mirror “M2” contains two disks represented by “vdev C” and “vdev D”.
Vdevs A, B, C, and D are all physical vdevs.
“M1” and M2” are logical vdevs;
they are also top-level vdevs since they originate from the “root vdev”.</p>
<figure>
<img src="Figures/zfs_vdev.svg" id="fig:vdev_sample" alt="Figure 1: Vdev Tree Sample Configuration" /><figcaption aria-hidden="true">Figure 1: Vdev Tree Sample Configuration</figcaption>
</figure>
</section>
<section id="vdev-labels" class="level2">
<h2>Vdev Labels</h2>
<p>Each physical vdev within a storage pool contains a 256KB structure called a <em>vdev label</em>.
The vdev label contains information
describing this particular physical vdev and all other vdevs
which share a common top-level vdev as an ancestor.
For example,
the vdev label structure contained on vdev “C”, in the previous illustration,
would contain information
describing the following vdevs:
“C”, “D”, and “M2”.
The contents of the vdev label are described in greater detail
in Section. 2.3,
Vdev Technical Details.</p>
<p>The vdev label serves two purposes:
it provides access to a pool’s contents
and it is used to verify a pool’s integrity and availability.
To ensure that the vdev label is always available and always valid,
redundancy and a staged update model are used.
To provide redundancy,
four copies of the label are written to each physical vdev within the pool.
The four copies are identical within a vdev,
but are not identical across vdevs in the pool.
During label updates,
a two staged transactional approach is used to ensure that
a valid vdev label is always available on disk.
Vdev label redundancy and the transactional update model are described in more detail below.</p>
<section id="label-redundancy" class="level3">
<h3>Label Redundancy</h3>
<p>Four copies of the vdev label are written to each physical vdev within a ZFS storage pool.
Aside from the small time frame during label update (described below),
these four labels are identical
and any copy can be used to access and verify the contents of the pool.
When a device is added to the pool,
ZFS places two labels at the front of the device and two labels at the back of the device.
Illustration. 2 shows the layout of these labels on a device of size N;
L0 and L1 represent the front two labels,
L2 and L3 represent the back two labels.</p>
<figure>
<img src="Figures/zfs_vdev_label.svg" id="fig:vdev_label" alt="Figure 2: Vdev Label layout on a block device of size N" /><figcaption aria-hidden="true">Figure 2: Vdev Label layout on a block device of size N</figcaption>
</figure>
<p>Based on the assumption that
corruption (or accidental disk overwrites) typically occurs in contiguous chunks,
placing the labels in non-contiguous locations (front and back) provides ZFS with a better probability
that some label will remain accessible in the case of media failure or accidental overwrite
(e.g. using the disk as a swap device while it is still part of a ZFS storage pool).</p>
</section>
<section id="transactional-two-staged-label-update" class="level3">
<h3>Transactional Two Staged Label Update</h3>
<p>The location of the vdev labels are fixed
at the time the device is added to the pool.
Thus,
the vdev label does not have copy-on-write semantics like everything else in ZFS.
Consequently,
when a vdev label is updated,
the contents of the label are overwritten.
Any time on-disk data is overwritten,
there is a potential for error.
To ensure that ZFS always has access to its labels,
a staged approach is used during update.
The first stage of the update writes the even labels (L0 and L2) to disk.
If, at any point in time,
the system comes down or faults during this update,
the odd labels will still be valid.
Once the even labels have made it out to stable storage,
the odd labels (L1 and L3) are updated and written to disk.
This approach has been carefully designed to ensure that
a valid copy of the label remains on disk at all times.</p>
</section>
</section>
<section id="sec:vdev_detail" class="level2">
<h2>Vdev Technical Details</h2>
<p>The contents of a vdev label are broken up into four pieces:
8KB of blank space,
8K of boot header information,
112KB of name-value pairs,
and 128KB of 1K sized uberblock structures.
The drawing below shows an expanded view of the L0 label.
A detailed description of each components follows:
blank space
(Section. 2.3.1),
boot block header
(Section. 2.3.2),
name/value pair list
(Section. 2.3.3),
and
uberblock array
array (Section. 2.3.4).</p>
<section id="sec:blankspace" class="level3">
<h3>Blank Space</h3>
<p>ZFS supports both VTOC (Volume Table of Contents) and EFI disk labels
as valid methods of describing disk layout.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>
While EFI labels are not written as part of a slice
(they have their own reserved space),
VTOC labels must be written to the first 8K of slice 0.
Thus,
to support VTOC labels,
the first 8k of the vdev_label is left empty to prevent potentially overwriting a VTOC disk label.</p>
</section>
<section id="sec:bootheader" class="level3">
<h3>Boot Block Header</h3>
<p>The boot block header is an 8K structure that is reserved for future use.
The contents of this block will be described in a future appendix of this paper.</p>
</section>
<section id="sec:nvlist" class="level3">
<h3>Name-Value Pair List</h3>
<p>The next 112KB of the label holds a collection of name-value pairs
describing this vdev and all of it’s <em>related vdevs</em>.
Related vdevs are defined as all vdevs within the subtree rooted at this vdev’s top-level vdev.
For example,
the vdev label on device “A”
(seen in Illustration. 1) would contain information
describing the subtree highlighted:
including vdevs “A”, “B”, and “M1” (top-level vdev).</p>
<p>All name-value pairs are stored in <em>XDR</em> encoded nvlists.
For more information on XDR encoding or nvlists,
see the <code>libnvpair</code> (3LIB) and <code>nvlist_free</code> (3NVPAIR) man pages.
The following name-value pairs (Table. 1)
are contained within this 112KB portion of the vdev_label.</p>
<div id="tbl:nvpair_vdev_label">
<table>
<caption>Table 1: Name-Value Pairs within vdev_label</caption>
<colgroup>
<col style="width: 12%" />
<col style="width: 14%" />
<col style="width: 23%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Value</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Version</td>
<td style="text-align: left;">``version”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">On disk format version.
Current value is “1” (5000?).</td>
</tr>
<tr class="even">
<td>Name</td>
<td style="text-align: left;">``name”</td>
<td style="text-align: left;">DATA_TYPE_STRING</td>
<td style="text-align: left;">Name of the pool in which this vdev belongs.</td>
</tr>
<tr class="odd">
<td>State</td>
<td style="text-align: left;">``state”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">State of this pool. The following table shows all
existing pool states. The Table. 2 shows
all existing pool states.</td>
</tr>
<tr class="even">
<td>Tranaction</td>
<td style="text-align: left;">``txg”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Transaction group number in which this label was
written to disk.</td>
</tr>
<tr class="odd">
<td>Pool Guid</td>
<td style="text-align: left;">``pool_guid”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Global unique identifier (guid) for the pool.</td>
</tr>
<tr class="even">
<td>Top Guid</td>
<td style="text-align: left;">``top_guid”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Global unique identifier for the top-level vdev
of this subtree.</td>
</tr>
<tr class="odd">
<td>Vdev Tree</td>
<td style="text-align: left;">``vdev_tree”</td>
<td style="text-align: left;">DATA_TYPE_NVLIST</td>
<td style="text-align: left;">The vdev_tree is a nvlist structure which is used
recursively to describe the hierarchical nature of
the vdev tree as seen in illustrations one and four.
The vdev_tree recursively describes each “related”
vdev witin this vdev’s subtree.</td>
</tr>
</tbody>
</table>
</div>
<div id="tbl:pool_states">
<table style="width:57%;">
<caption>Table 2: Pool States</caption>
<colgroup>
<col style="width: 38%" />
<col style="width: 18%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">State</th>
<th style="text-align: left;">Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">POOL_STATE_ACTIVE</td>
<td style="text-align: left;">0</td>
</tr>
<tr class="even">
<td style="text-align: left;">POOL_STATE_EXPORTED</td>
<td style="text-align: left;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">POOL_STATE_DESTROYED</td>
<td style="text-align: left;">2</td>
</tr>
</tbody>
</table>
</div>
<p>Each vdev_tree nvlist contains the elements as described in the Table. 3.
Note that not all nvlist elements are applicable to all vdevs types.
Therefore,
a vdev_tree nvlist may contain only a subset of the elements described below.
The Illustration. 3 below shows what the ``vdev_tree” entry might look
like for “vdev A” as shown in Illustration. 2
earlier in this document.</p>
<figure>
<img src="Figures/zfs_vdev_tree.svg" id="fig:vdev_tree" alt="Figure 3: Vdev Tree Nvlist Entries" /><figcaption aria-hidden="true">Figure 3: Vdev Tree Nvlist Entries</figcaption>
</figure>
<div id="tbl:vdevtree_nvlist_entries">
<table>
<caption>Table 3: Vdev Tree NV List Entries</caption>
<colgroup>
<col style="width: 18%" />
<col style="width: 31%" />
<col style="width: 49%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Name</th>
<th style="text-align: left;">Value</th>
<th style="text-align: left;">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">``type”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">The id is the index of this vdev in its parent’s
children array.</td>
</tr>
<tr class="even">
<td style="text-align: left;">``guid”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Global Unique Identifier for this vdev_tree element.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">``path”</td>
<td style="text-align: left;">DATA_TYPE_STRING</td>
<td style="text-align: left;">Device path. Only used for leaf vdevs.</td>
</tr>
<tr class="even">
<td style="text-align: left;">``devid”</td>
<td style="text-align: left;">DATA_TYPE_STRING</td>
<td style="text-align: left;">Device ID for this vdev_tree element. Only used for
vdevs of type disk.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">“metaslab_array”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Object number of an object containing an array of object
numbers. Each element of this array (ma[i]) is, in turn,
an object number of a space map for metaslab ‘i’.</td>
</tr>
<tr class="even">
<td style="text-align: left;">“metaslab_shift”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Log base 2 of the metaslab size.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">“ashift”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Log base 2 of the minimum allocatable unit for this
top level vdev. This is currently ‘10’ for a RAIDz
configuration, `9’ otherwise.</td>
</tr>
<tr class="even">
<td style="text-align: left;">“asize”</td>
<td style="text-align: left;">DATA_TYPE_UINT64</td>
<td style="text-align: left;">Amount of space that can be allocated from this top level
vdev</td>
</tr>
<tr class="odd">
<td style="text-align: left;">“children”</td>
<td style="text-align: left;">DATA_TYPE_NVLIST_ARRAY</td>
<td style="text-align: left;">Array of vdev_tree nvlists for each child of this
vdev_tree element.</td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="sec:ub" class="level3">
<h3>The Uberblock</h3>
<p>Immediately following the nvpair lists in the vdev label is an array of <em>uberblocks</em>.
The uberblock is the portion of the label
containing information necessary to access the contents of the pool<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>.
Only one uberblock in the pool is active at any point in time.
The uberblock with the highest transaction group number and valid SHA-256 checksum
is the active uberblock.</p>
<p>To ensure constant access to the active uberblock,
the active uberblock is never overwritten.
Instead, all updates to an uberblock are done
by writing a modified uberblock to another element of the uberblock array.
Upon writing the new uberblock,
the transaction group number and timestamps are incremented
thereby making it the new active uberblock in a single atomic action.
Uberblocks are written in a round robin fashion across the various vdevs with the pool.
The Illustration. 4 has an expanded view of two uberblocks within an uberblock array.</p>
<figure>
<img src="Figures/zfs_ub_expanded.svg" id="fig:ub_expanded" alt="Figure 4: Uberblock array showing uberblock contents" /><figcaption aria-hidden="true">Figure 4: Uberblock array showing uberblock contents</figcaption>
</figure>
<section id="uberblock-technical-details" class="level4">
<h4>Uberblock Technical Details</h4>
<p>The uberblock is stored in the machine’s native endian format and has the following contents:</p>
<ul>
<li><p><strong>ub_magic:</strong>
The uberblock magic number is a 64 bit integer
used to identify a device as containing ZFS data.
The value of the ub_magic is <span class="math inline">0x00bab10c</span> (oo-ba-block).
The Table. 4 shows the ub_magic number as seen on disk.</p>
<div id="tbl:endianess_ub">
<table style="width:56%;">
<caption>Table 4: Uberblock values per machine endian type</caption>
<colgroup>
<col style="width: 31%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Machine Endianess</th>
<th style="text-align: left;">Uberblock Value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Big Endian</td>
<td style="text-align: left;"><code>0x00bab10c</code></td>
</tr>
<tr class="even">
<td style="text-align: left;">Little Endian</td>
<td style="text-align: left;"><code>0x0cb1ba00</code></td>
</tr>
</tbody>
</table>
</div></li>
<li><p><strong>ub_version:</strong>
The version field is used to identify the on-disk format in which this data is laid out.
The current on-disk format version number is 0x1 (5000?).
This field contains the same value
as the “version” element of the name/value pairs described in Section. 2.3.3.</p></li>
<li><p><strong>ub_txg:</strong>
All writes in ZFS are done in transaction groups.
Each group has an associated transaction group number.
The ub_txg value reflects the transaction group in which
this uberblock was written.
The ub_txg number must be greater than or equal to the “txg” number
stored in the nvlist for this label to be valid.</p></li>
<li><p><strong>ub_guid_sum:</strong>
The ub_guid_sum is used to verify the availability of vdevs within a pool.
When a pool is opened,
ZFS traverses all leaf vdevs within the pool
and totals a running sum of all the GUIDs
(a vdev’s guid is stored in the guid nvpair entry, see Section. 2.3.3) it encounters.
This computed sum is checked against the ub_guid_sum
to verify the availability of all vdevs within this pool.</p></li>
<li><p><strong>ub_timestamp:</strong>
Coordinated Universal Time (UTC) when
this uberblock wasf written in seconds since January 1st 1970 (GMT).</p></li>
<li><p><strong>ub_rootbp:</strong>
The ub_rootbp is a blkptr structure containing the location of the MOS.
Both the MOS and blkptr structures are described in later chapters of this document:
Chapters 4 and 2 respectively.</p></li>
</ul>
</section>
</section>
</section>
<section id="boot-block" class="level2">
<h2>Boot Block</h2>
<p>Immediately following the L0 and L1 labels is a 3.5MB chunk reserved for future use
(see Illustration. 2).
The contents of this block will be described in a future appendix of this paper.</p>
</section>
</section>
<section id="block-pointers-and-indirect-blocks" class="level1">
<h1>Block Pointers and Indirect Blocks</h1>
<p>Data is transferred between disk and main memory in units called blocks.
A block pointer (<code>blkptr_t</code>) is a 128 byte ZFS structure used
to physically locate, verify, and describe blocks of data on disk.</p>
<p>The 128 byte blkptr_t structure layout is shown in the Illustration. 5.</p>
<figure>
<img src="Figures/zfs_blkptr.svg" id="fig:blkptr" alt="Figure 5: Block pointer structure showing byte by byte usage." /><figcaption aria-hidden="true">Figure 5: Block pointer structure showing byte by byte usage.</figcaption>
</figure>
<p>Normally,
block pointers point (via their DVAs) to a block which holds data.
If the data that we need to store is very small,
this is an inefficient use of space,
Additionally, reading these small blocks tends to generate
more random reads.
Embedded-data Block Pointers was introduced.
It allows small pieces of data
(the “payload”, upto 112 bytes) embedded in the block pointer,
the block pointer doesn’t point to anything then.
The layout of an embedded block pointer is as Illustration. 6.</p>
<figure>
<img src="Figures/zfs_embedded_blkptr.svg" id="fig:embedded" alt="Figure 6: Embedded Block Pointer Layout" /><figcaption aria-hidden="true">Figure 6: Embedded Block Pointer Layout</figcaption>
</figure>
<section id="data-virtual-address" class="level2">
<h2>Data Virtual Address</h2>
<p>The <em>data virtual address</em>, or <em>DVA</em> is the name
given to the combination of the vdev and offset portions of the block pointer,
for example the combination of vdev1 and offset1 make up a DVA (dva1).
ZFS provides the capability of storing up to three copies of the data pointed to by the block pointer,
each pointed to by a unique DVA (dva1, dva2, or dva3).
The data stored in each of these copies is identical.
The number of DVAs used per block pointer is purely a policy decision
and is called the “wideness” of the block pointer:
single wide block pointer (1 DVA),
double wide block pointer (2 DVAs),
and triple wide block pointer (3 DVAs).</p>
<p>The <em>vdev</em> portion of each DVA is a 32 bit integer
which uniquely identifies the vdev ID containing this block.
The offset portion of the DVA is a 63 bit integer value
holding the offset (starting after the vdev labels (L0 and L1) and boot block)
within that device where the data lives.
Together,
the vdev and offset uniquely identify the block address of the data it points to.</p>
<p>The value stored in offset is the offset in terms of sectors (512 byte blocks).
To find the physical block byte offset from the beginning of a slice,
the value inside offset must be shifted over (<span class="math inline">\ll</span>) by <span class="math inline">9</span> (<span class="math inline">2^9 =512</span>)
and this value must be added to <span class="math inline">0x400000</span>
(size of two vdev_labels and boot block).</p>
<p><span class="math display">
physical\ block\ address = (\mathit{offset} \ll 9) + 0x400000~(4MB)
</span></p>
</section>
<section id="grid" class="level2">
<h2>GRID</h2>
<p>Raid-Z layout information, reserved for future use.</p>
</section>
<section id="gang" class="level2">
<h2>GANG</h2>
<p>A <em>gang block</em> is a block whose contents contain block pointers.
Gang blocks are used when the amount of space requested is not available in a contiguous block.
In a situation of this kind,
several smaller blocks will be allocated
(totaling up to the size requested)
and a gang block will be created to contain the block pointers for the allocated blocks.
A pointer to this gang block is returned to the requester,
giving the requester the perception of a single block.</p>
</section>
</section>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p>Disk labels describe disk partition and slice information.
See <code>fdisk</code>(1M) and/or <code>format</code>(1M) for more information on disk partitions and slices.
It should be noted that disk labels are a completely separate entity from vdev labels
and while their naming is similar,
they should not be confused as being similar.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p>The uberblock is similar to the superblock in UFS.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</article>
</body>
</html>
